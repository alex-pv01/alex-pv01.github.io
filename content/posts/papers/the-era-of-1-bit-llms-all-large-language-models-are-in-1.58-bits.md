---
title: "Notes on The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits"
date: 2024-04-11T15:46:04+02:00
draft: true
math: true

cover:
    image: "<image path/url>"
    # can also paste direct link from external site
    # ex. https://i.ibb.co/K0HVPBd/paper-mod-profilemode.png
    alt: "<alt text>"
    caption: "<text>"
    relative: false # To use relative path for cover image, used in hugo Page-bundles

tags: []

ShowToc: false
---

***Disclaimer:*** *This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.*

The following post is a comment on the paper [The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits](#1) by [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+S), [Hongyu Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+H), [Lingxiao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+L), [Lei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+L), [Wenhui Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+S), [Li Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong,+L), [Ruiping Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+R), [Jilong Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue,+J), and [Furu Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+F).





### References
<a id="1">[1]</a> Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., & Wei, F. (2024). The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits. [arXiv:2402.17764](https://arxiv.org/abs/2402.17764).