<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Àlex</title>
    <link>https://alex-pv01.github.io/posts/</link>
    <description>Recent content in Posts on Àlex</description>
    <image>
      <title>Àlex</title>
      <url>https://alex-pv01.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://alex-pv01.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 09 Apr 2024 10:13:13 +0200</lastBuildDate><atom:link href="https://alex-pv01.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on Mixed-Privacy Forgetting in Deep Networks</title>
      <link>https://alex-pv01.github.io/posts/papers/mixed-privacy-forgetting-in-deep-networks/</link>
      <pubDate>Tue, 09 Apr 2024 10:13:13 +0200</pubDate>
      
      <guid>https://alex-pv01.github.io/posts/papers/mixed-privacy-forgetting-in-deep-networks/</guid>
      <description>Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper Mixed-Privacy Forgetting in Deep Networks by Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto.
Golatkar et. al. introduce a novel method for forgetting in a mixed-privacy setting, where a core subset of the training samples will not be forgotten.</description>
    </item>
    
    <item>
      <title>Notes on Multi Class Explainable Unlearning for Image Classification via Weight Filtering</title>
      <link>https://alex-pv01.github.io/posts/papers/multi-class-explainable-unlearning-for-image-classification-via-weight-filtering/</link>
      <pubDate>Mon, 08 Apr 2024 09:36:22 +0200</pubDate>
      
      <guid>https://alex-pv01.github.io/posts/papers/multi-class-explainable-unlearning-for-image-classification-via-weight-filtering/</guid>
      <description>Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper Multi Class Explainable Unlearning for Image Classification via Weight Filtering by Samuele Poppi, Sara Sarto, Marcella Cornia, Lorenzo Baraldi and Rita Cucchiara.
Samuele P., et. al.</description>
    </item>
    
    <item>
      <title>Attention and Context based Embeddings</title>
      <link>https://alex-pv01.github.io/posts/2022-12-17-attention/</link>
      <pubDate>Sat, 17 Dec 2022 17:51:02 +0100</pubDate>
      
      <guid>https://alex-pv01.github.io/posts/2022-12-17-attention/</guid>
      <description>Attention mechanisms are a type of techniques used in natural language processing (NLP) tasks that allow a model to focus on specific parts of the input when processing a sequence, rather than considering the entire sequence at once. These methods can improve the performance of the model by allowing it to efficiently process long sequences of text and make more accurate predictions.
To some extend, attention mechanisms are motivated by how human visual attention focuses on different regions of an image or how correlates words in a sentence.</description>
    </item>
    
  </channel>
</rss>
