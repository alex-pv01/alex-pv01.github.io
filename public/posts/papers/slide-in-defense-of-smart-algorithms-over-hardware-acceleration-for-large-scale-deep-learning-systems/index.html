<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css integrity=sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js integrity=sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js integrity=sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems | Àlex</title>
<meta name=keywords content="Deep Learning,Hardware Acceleration,Locality Sensitive Hashing,Adaptive Sampling,SLIDE,Parallelization,Sparse Neural Networks,Research,Paper"><meta name=description content="Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems by Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai and Anshumali Shrivastava, from Rice University and Intel Corporation."><meta name=author content="Àlex Pujol Vidal"><link rel=canonical href=http://localhost:1313/posts/papers/slide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/papers/slide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><!doctype html><html><head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head>...</html><meta property="og:title" content="Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems"><meta property="og:description" content="Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems by Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai and Anshumali Shrivastava, from Rice University and Intel Corporation."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/papers/slide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems/"><meta property="og:image" content="http://localhost:1313/%3Cimage%20path/url%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-21T18:45:21+02:00"><meta property="article:modified_time" content="2024-04-21T18:45:21+02:00"><meta property="og:site_name" content="Àlex' personal site"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Cimage%20path/url%3E"><meta name=twitter:title content="Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems"><meta name=twitter:description content="Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems by Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai and Anshumali Shrivastava, from Rice University and Intel Corporation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems","item":"http://localhost:1313/posts/papers/slide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems","name":"Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems","description":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems by Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai and Anshumali Shrivastava, from Rice University and Intel Corporation.","keywords":["Deep Learning","Hardware Acceleration","Locality Sensitive Hashing","Adaptive Sampling","SLIDE","Parallelization","Sparse Neural Networks","Research","Paper"],"articleBody":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems by Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai and Anshumali Shrivastava, from Rice University and Intel Corporation.\nTo get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, such machines are expensive and hard to generalize to a multitude of tasks. In this paper, the authors propose SLIDE (Sub-Linear Deep learning Engine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. The authors show that SLIDE on a 44-core CPU, drastically reduces the computations during both training (3.5 times faster) and inference outperforming an optimized implementation of Tensorflow on a Tesla V100 GPU.\nThey explore the idea of adaptative sparzity. The idea stems from the fact that we can accurately train neural networks by selectively sparsifying most of the neurons, based on their activation, during every gradient update. However, this technique does not directly lead to computational savings. To achieve that, they employ Locality Sensitive Hash (LSH) tables to identify a sparse neurons efficiently during each step.\nLocality Sensivite Hashing LSH is a family of functions with the property that similar input objects in the domain of these functions have a higher probability of colliding in the range space than non-similar ones.\nIt is shown that having an LSH family for a given similarity measure, is sufficient for efficiently solving nearest-neighbor search in sub-linear time.\nLSH Algorithm The LSH algorithm uses two parameters $(K, L)$. Authors contruct $L$ independent hash tables, each of which has a meta-hash function $H$ that is formed by concatenating $K$ random independent hash functions. Given a query, we collect one bucket of each table and return the union of all $L$ buckets, which reduces the number of false negatives. Only valid nearest-neighbor items are likely to match all $K$ hash values for a given query, thus $H$ reduces the number of false positives:\nPre-processing phase: Construct $L$ hash tables, storing the pointers to the data elements. Query phase: Given a query $Q$, search for its nearest-neighbors by querying all $L$ hash tables and returning the union of all $L$ buckets. LSH for Estimation and Sampling It turns out that taking a few hash buckets (as low as 1) is sufficient for adaptive sampling. Given a collection of vectors $\\mathcal{C}$ and a query $Q$, we get a candidate set $S$ from a $(K,L)$-LSH algorithm. Every element $x_i \\in \\mathcal{C}$ gets sampled into $S$ with probability $p_i$, where $p_i$ is a monotonically inicreasing function of $Q \\cdot x_i$. Thus, we can pay on-time linear cost of preprocessing $\\mathcal{C}$ into hash tables, and adaptive sampling for quary $Q$ only requires few hash lookups.\nSLIDE Algorithm Initialization: Every layer object contains a list of neurons and a set size $L$ of LSH sampling hash tables. Each hash table contains $K$ LSH hash functions and the ids of the neurons that are hashed into the buckets. The weights of the network are initialized randomly. If $h_i^l$ is the $i$-th hash function in the $l$-th layer, the hash value of the $j$-th neuron’s weights in the $l$-th layer is denoted by $h_i^l(\\omega_l^j)$. After compunting the hash values for the $K$ hash functions, the id $j$ of the neuron is inserted into the corresponding buckets of the hash table in the $l$-th layer. This is done for all neurons in all layers. Note that this is easily parallelizable.\nSparse Feed-Forward Pass with Hash Table Sampling: The input of each layer $\\textbf{x}_l$ is fed into the hash functions $h_i^l$ to get the hash values. The active neurons are then sampled by querying the has tables and retriving the ids from the matching buckets. Only the activations of active neurons are computed and passed as inputs to the next layer. The other activations are treated as zeros and are never computed. Efficiently sampling the active neurons is crucial for the performance of the algorithm. If $\\beta_l$ is the number of active neuros we target to retrive at layer $l$, authors propose three strategies to sample the active neurons:\nVanilla Sampling: Randomly choose one of the $L$ hash tables and query the corresponding bucket. Keep repeating this process until $\\beta_l$ neurons are sampled. Top-$\\beta$ Sampling: Aggregate the number of neurons in all buckets of all hash tables and select the top $\\beta_l$ neurons. Hard Thresholding: Set a threshold $m$ and proceed similarly as in Top-$\\beta$ Sampling, but only select the neurons that have more than $m$ neurons in their bucket, thus avoiding the need to query all hash tables. Sparse Backpropagation or Gradient Update: Classical backpropagation layer-by-layer is used to compute the gradients of the loss function with respect to the weights of the network, rather than vector-based backpropagation. As a result, non-active neurons are never accessed and the computation cost is only proportional to the number of active neurons.\nUpdate Hash Tables: After modifying the weights of the network, the hash values of the neurons are recomputed and the hash tables are updated accordingly. This can be computationally expensive, but authors propose some simple strategies to reduce the cost:\nExponentially Delayed Update: Instead of updating the hash tables after every gradient update, the authors propose to update the hash tables after a certain number of gradient updates. Starting with a delay of $T_0$ iterations for the update, authors apply exponential decay to the delay, i.e. the $t$-th update occurs after $\\sum_{i=0}^{t-1} T_0e^{-\\lambda i}$ iterations, where $\\lambda$ is a decay factor. Replace Policy: Buckets have a maximum capacity. To decide which neuron to replace autors propose to policies. On one hand, the Reservoir Policy that uses Vitters reservoir sampling. On the other hand, the First-In-First-Out (FIFO) Policy that replaces the oldest neuron in the bucket. OpenMP Parallelization across a Batch: To ensure the independence of computation across different threads, every neuron stores three additional arrays, each of whose length is equal to the batch size. These arrays keep track of the input specific activations, gradients and active input neurons. Authors claim that is memory overhead is negligible for CPUs. The extreme sparsity and randomness in gradient updates allow to asynchronously parallelize the computation of the gradients across the batch, without a considerable amount of overlapping updates. Details of Hash Functions and Hash Tables There is a trade-off between efficiency of retrieving active neuros and the quality of the retrived ones. Authors propose using four different types of hash functions from LSH family Simhash, WTA hash, DWTA hash and Minhash.\nExperimental Results Comparison against SLIDE, TF-GPU and TF-CPU. Note that the time required for convergence in SLIDE is the lowest, while comparing against the number of iterations the behaviour looks identical, which suggests that SLIDE superiority is due to algorithm and implementation and not due to any optimizations tricks.\nSLIDE outperforms the baselines at all batch sizes, and even the gap gets wider as batch size increases.\nComparison of performance gains with the number of CPU cores. Convergence time droops steeply for SLIDE as the number of cores grow.\nConclusion SLIDE is a novel algorithm that combines smart randomized algorithms with the right data structures that allow asynchronous parallelization across a batch. The authors show that SLIDE outperforms 3.5 times faster than TF-GPU on a Tesla V100 GPU.\nReferences [1] Chen, B., Medini, T., Farwell, J., Gobriel, S., Tai, C., \u0026 Shrivastava, A. (2024). SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems. arXiv:1903.03129.\n","wordCount":"1291","inLanguage":"en","image":"http://localhost:1313/%3Cimage%20path/url%3E","datePublished":"2024-04-21T18:45:21+02:00","dateModified":"2024-04-21T18:45:21+02:00","author":{"@type":"Person","name":"Àlex Pujol Vidal"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/papers/slide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems/"},"publisher":{"@type":"Organization","name":"Àlex","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Àlex (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Àlex</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://www.countop.com/ title=countop.com><span>countop.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems</h1><div class=post-meta><span title='2024-04-21 18:45:21 +0200 CEST'>April 21, 21216</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Àlex Pujol Vidal&nbsp;|&nbsp;<a href=https://github.com/alex-pv01/alex-pv01.github.io/tree/main/content/posts/papers/slide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p><em><strong>Disclaimer:</strong></em> <em>This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.</em></p><p>The following post is a comment on the paper <a href=#1>SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems</a> by <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+B">Beidi Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Medini,+T">Tharun Medini</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Farwell,+J">James Farwell</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gobriel,+S">Sameh Gobriel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tai,+C">Charlie Tai</a> and <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shrivastava,+A">Anshumali Shrivastava</a>, from <a href=https://www.rice.edu/>Rice University</a> and <a href=https://www.intel.com/content/www/us/en/homepage.html>Intel Corporation</a>.</p><p>To get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, such machines are expensive and hard to generalize to a multitude of tasks. In this paper, the authors propose <strong>SLIDE</strong> (<strong>S</strong>ub-<strong>Li</strong>near <strong>D</strong>eep learning <strong>E</strong>ngine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. The authors show that SLIDE on a 44-core CPU, drastically reduces the computations during both training (3.5 times faster) and inference outperforming an optimized implementation of Tensorflow on a Tesla V100 GPU.</p><p>They explore the idea of <strong>adaptative sparzity</strong>. The idea stems from the fact that we can accurately train neural networks by selectively sparsifying most of the neurons, based on their activation, during every gradient update. However, this technique does not directly lead to computational savings. To achieve that, they employ <strong>Locality Sensitive Hash</strong> (LSH) tables to identify a sparse neurons efficiently during each step.</p><h2 id=locality-sensivite-hashing>Locality Sensivite Hashing<a hidden class=anchor aria-hidden=true href=#locality-sensivite-hashing>#</a></h2><p>LSH is a family of functions with the property that similar input objects in the domain of these functions have a higher <strong>probability of colliding</strong> in the range space than non-similar ones.</p><p>It is shown that having an LSH family for a given similarity measure, is sufficient for efficiently solving nearest-neighbor search in sub-linear time.</p><h3 id=lsh-algorithm>LSH Algorithm<a hidden class=anchor aria-hidden=true href=#lsh-algorithm>#</a></h3><p>The LSH algorithm uses two parameters $(K, L)$. Authors contruct $L$ independent hash tables, each of which has a meta-hash function $H$ that is formed by concatenating $K$ random independent hash functions. Given a query, we collect one bucket of each table and return the union of all $L$ buckets, which reduces the number of false negatives. Only valid nearest-neighbor items are likely to match all $K$ hash values for a given query, thus $H$ reduces the number of false positives:</p><ol><li><strong>Pre-processing phase:</strong> Construct $L$ hash tables, storing the pointers to the data elements.</li><li><strong>Query phase:</strong> Given a query $Q$, search for its nearest-neighbors by querying all $L$ hash tables and returning the union of all $L$ buckets.</li></ol><h3 id=lsh-for-estimation-and-sampling>LSH for Estimation and Sampling<a hidden class=anchor aria-hidden=true href=#lsh-for-estimation-and-sampling>#</a></h3><p>It turns out that taking a few hash buckets (as low as 1) is sufficient for adaptive sampling. Given a collection of vectors $\mathcal{C}$ and a query $Q$, we get a candidate set $S$ from a $(K,L)$-LSH algorithm. Every element $x_i \in \mathcal{C}$ gets sampled into $S$ with probability $p_i$, where $p_i$ is a monotonically inicreasing function of $Q \cdot x_i$. Thus, we can pay on-time linear cost of preprocessing $\mathcal{C}$ into hash tables, and adaptive sampling for quary $Q$ only requires few hash lookups.</p><h2 id=slide-algorithm>SLIDE Algorithm<a hidden class=anchor aria-hidden=true href=#slide-algorithm>#</a></h2><ol><li><strong>Initialization:</strong> Every layer object contains a list of neurons and a set size $L$ of LSH sampling hash tables. Each hash table contains $K$ LSH hash functions and the ids of the neurons that are hashed into the buckets. The weights of the network are initialized randomly.</li></ol><p>If $h_i^l$ is the $i$-th hash function in the $l$-th layer, the hash value of the $j$-th neuron&rsquo;s weights in the $l$-th layer is denoted by $h_i^l(\omega_l^j)$. After compunting the hash values for the $K$ hash functions, the id $j$ of the neuron is inserted into the corresponding buckets of the hash table in the $l$-th layer. This is done for all neurons in all layers. Note that this is easily parallelizable.</p><ol start=2><li><strong>Sparse Feed-Forward Pass with Hash Table Sampling:</strong>
The input of each layer $\textbf{x}_l$ is fed into the hash functions $h_i^l$ to get the hash values. The <strong>active neurons</strong> are then sampled by querying the has tables and retriving the ids from the matching buckets. Only the activations of active neurons are computed and passed as inputs to the next layer. The other activations are treated as zeros and are never computed.</li></ol><p>Efficiently sampling the active neurons is crucial for the performance of the algorithm. If $\beta_l$ is the number of active neuros we target to retrive at layer $l$, authors propose three strategies to sample the active neurons:</p><ul><li><strong>Vanilla Sampling:</strong> Randomly choose one of the $L$ hash tables and query the corresponding bucket. Keep repeating this process until $\beta_l$ neurons are sampled.</li><li><strong>Top-$\beta$ Sampling:</strong> Aggregate the number of neurons in all buckets of all hash tables and select the top $\beta_l$ neurons.</li><li><strong>Hard Thresholding:</strong> Set a threshold $m$ and proceed similarly as in Top-$\beta$ Sampling, but only select the neurons that have more than $m$ neurons in their bucket, thus avoiding the need to query all hash tables.</li></ul><ol start=3><li><p><strong>Sparse Backpropagation or Gradient Update:</strong>
Classical backpropagation layer-by-layer is used to compute the gradients of the loss function with respect to the weights of the network, rather than vector-based backpropagation. As a result, non-active neurons are never accessed and the computation cost is only proportional to the number of active neurons.</p></li><li><p><strong>Update Hash Tables:</strong>
After modifying the weights of the network, the hash values of the neurons are recomputed and the hash tables are updated accordingly. This can be computationally expensive, but authors propose some simple strategies to reduce the cost:</p></li></ol><ul><li><strong>Exponentially Delayed Update:</strong> Instead of updating the hash tables after every gradient update, the authors propose to update the hash tables after a certain number of gradient updates. Starting with a delay of $T_0$ iterations for the update, authors apply <strong>exponential decay</strong> to the delay, i.e. the $t$-th update occurs after $\sum_{i=0}^{t-1} T_0e^{-\lambda i}$ iterations, where $\lambda$ is a decay factor.</li><li><strong>Replace Policy:</strong> Buckets have a maximum capacity. To decide which neuron to replace autors propose to policies. On one hand, the <strong>Reservoir Policy</strong> that uses <a href=https://en.wikipedia.org/wiki/Reservoir_sampling>Vitters reservoir sampling</a>. On the other hand, the <strong>First-In-First-Out (FIFO) Policy</strong> that replaces the oldest neuron in the bucket.</li></ul><ol start=5><li><strong>OpenMP Parallelization across a Batch:</strong>
To ensure the independence of computation across different threads, every neuron stores three additional arrays, each of whose length is equal to the batch size. These arrays keep track of the input specific activations, gradients and active input neurons. Authors claim that is memory overhead is negligible for CPUs.
The extreme sparsity and randomness in gradient updates allow to asynchronously parallelize the computation of the gradients across the batch, without a considerable amount of overlapping updates.</li></ol><h3 id=details-of-hash-functions-and-hash-tables>Details of Hash Functions and Hash Tables<a hidden class=anchor aria-hidden=true href=#details-of-hash-functions-and-hash-tables>#</a></h3><p>There is a trade-off between <strong>efficiency</strong> of retrieving active neuros and the <strong>quality</strong> of the retrived ones. Authors propose using four different types of hash functions from LSH family <a href=https://www.vldb.org/conf/1999/P49.pdf>Simhash</a>, <a href=https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37298.pdf>WTA hash</a>, <a href=http://auai.org/uai2018/proceedings/papers/321.pdf>DWTA hash</a> and <a href=https://web.archive.org/web/20150131043133/http://gatekeeper.dec.com/ftp/pub/dec/SRC/publications/broder/positano-final-wpnums.pdf>Minhash</a>.</p><h2 id=experimental-results>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results>#</a></h2><p><figure><img loading=lazy src=/figures/slide/figure-5a.png alt="SLIDE Performance"></figure><figure><img loading=lazy src=/figures/slide/figure-5b.png alt="SLIDE Performance"><figcaption><p>Comparison against SLIDE, TF-GPU and TF-CPU. Note that the time required for convergence in SLIDE is the lowest, while comparing against the number of iterations the behaviour looks identical, which suggests that SLIDE superiority is due to algorithm and implementation and not due to any optimizations tricks.</p></figcaption></figure></p><br><figure><img loading=lazy src=/figures/slide/figure-8.png alt="SLIDE Performance"><figcaption><p>SLIDE outperforms the baselines at all batch sizes, and even the gap gets wider as batch size increases.</p></figcaption></figure><br><figure><img loading=lazy src=/figures/slide/figure-9.png alt="SLIDE Performance"><figcaption><p>Comparison of performance gains with the number of CPU cores. Convergence time droops steeply for SLIDE as the number of cores grow.</p></figcaption></figure><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>SLIDE is a novel algorithm that combines smart randomized algorithms with the right data structures that allow asynchronous parallelization across a batch. The authors show that SLIDE outperforms 3.5 times faster than TF-GPU on a Tesla V100 GPU.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Chen, B., Medini, T., Farwell, J., Gobriel, S., Tai, C., & Shrivastava, A. (2024). <em>SLIDE: In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems</em>. <a href=https://arxiv.org/abs/1903.03129>arXiv:1903.03129</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/deep-learning/>Deep Learning</a></li><li><a href=http://localhost:1313/tags/hardware-acceleration/>Hardware Acceleration</a></li><li><a href=http://localhost:1313/tags/locality-sensitive-hashing/>Locality Sensitive Hashing</a></li><li><a href=http://localhost:1313/tags/adaptive-sampling/>Adaptive Sampling</a></li><li><a href=http://localhost:1313/tags/slide/>SLIDE</a></li><li><a href=http://localhost:1313/tags/parallelization/>Parallelization</a></li><li><a href=http://localhost:1313/tags/sparse-neural-networks/>Sparse Neural Networks</a></li><li><a href=http://localhost:1313/tags/research/>Research</a></li><li><a href=http://localhost:1313/tags/paper/>Paper</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/lectures/nonequilibrium-statistical-mechanics/><span class=title>« Prev</span><br><span>Lecture Notes on Nonequilibrium Statistical Mechanics</span>
</a><a class=next href=http://localhost:1313/posts/papers/the-era-of-1-bit-llms-all-large-language-models-are-in-1.58-bits/><span class=title>Next »</span><br><span>Notes on The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on x" href="https://x.com/intent/tweet/?text=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f&amp;hashtags=DeepLearning%2cHardwareAcceleration%2cLocalitySensitiveHashing%2cAdaptiveSampling%2cSLIDE%2cParallelization%2cSparseNeuralNetworks%2cResearch%2cPaper"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f&amp;title=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems&amp;summary=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f&title=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on whatsapp" href="https://api.whatsapp.com/send?text=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on telegram" href="https://telegram.me/share/url?text=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Slide in Defense of Smart Algorithms Over Hardware Acceleration for Large Scale Deep Learning Systems on ycombinator" href="https://news.ycombinator.com/submitlink?t=Notes%20on%20Slide%20in%20Defense%20of%20Smart%20Algorithms%20Over%20Hardware%20Acceleration%20for%20Large%20Scale%20Deep%20Learning%20Systems&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fpapers%2fslide-in-defense-of-smart-algorithms-over-hardware-acceleration-for-large-scale-deep-learning-systems%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Àlex</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>