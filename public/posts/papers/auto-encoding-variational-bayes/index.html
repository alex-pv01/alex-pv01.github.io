<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes on Auto Encoding Variational Bayes | Àlex</title>
<meta name=keywords content="Variational Inference,Variational Autoencoder,Autoencoder,Latent Space,SGVB,AEVB,VAE,AI,Machine Learning,Deep Learning,Probabilistic Models,Probabilistic Graphical Models,PGM,Directed Models,Paper,Research"><meta name=description content="Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
What introduces their contributions is the following question:
How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?"><meta name=author content="Àlex Pujol Vidal"><link rel=canonical href=https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://alex-pv01.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alex-pv01.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alex-pv01.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://alex-pv01.github.io/apple-touch-icon.png><link rel=mask-icon href=https://alex-pv01.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><!doctype html><html><head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css integrity=sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js integrity=sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js integrity=sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head>...</html><meta property="og:title" content="Notes on Auto Encoding Variational Bayes"><meta property="og:description" content="Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
What introduces their contributions is the following question:
How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?"><meta property="og:type" content="article"><meta property="og:url" content="https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/"><meta property="og:image" content="https://alex-pv01.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-10T12:34:09+02:00"><meta property="article:modified_time" content="2024-04-10T12:34:09+02:00"><meta property="og:site_name" content="Àlex' personal site"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://alex-pv01.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Notes on Auto Encoding Variational Bayes"><meta name=twitter:description content="Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.
The following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.
What introduces their contributions is the following question:
How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://alex-pv01.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Notes on Auto Encoding Variational Bayes","item":"https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes on Auto Encoding Variational Bayes","name":"Notes on Auto Encoding Variational Bayes","description":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.\nWhat introduces their contributions is the following question:\nHow can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?","keywords":["Variational Inference","Variational Autoencoder","Autoencoder","Latent Space","SGVB","AEVB","VAE","AI","Machine Learning","Deep Learning","Probabilistic Models","Probabilistic Graphical Models","PGM","Directed Models","Paper","Research"],"articleBody":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.\nWhat introduces their contributions is the following question:\nHow can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?\nThey show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound, which they call Stochastic Gradient Variational Bayes (SGVB) estimator. It can be used for efficient approximate posterior inference and learning in directed probabilistic models with continuous latent variables.\nFor the case of i.i.d. dataset and continuous latent variables, they propose the Auto-Encoding Variational Bayes (AEVB) algorithm. They use the SGVB estimator to optimize a recognition model that allows for efficient approximate posterior inference. This model derives to a Variational Autoencoder (VAE), which is a neural network-based recognition model.\nMethod Problem Statement Given a dataset $\\mathcal{D} = \\{\\textbf{x}^{(i)}\\}_{i=1}^N$ of $N$ i.i.d. samples of some continuous or discrete variable $\\textbf{x}$, and assuming that the data is generated by a random process, involving the continuous random variable $\\textbf{z}$, the generative process consists of two steps:\nA value $\\textbf{z}^{(i)}$ is generated from a prior distribution $p_{\\theta^*}(\\textbf{z})$.\nA value $\\textbf{x}^{(i)}$ is generated from a conditional distribution $p_{\\theta^*}(\\textbf{x}|\\textbf{z})$.\nParameters $\\theta^*$ and the latent variables $\\textbf{z}$ are unknown. The goal is to define an algorithm to find an approximation posterior inference of $\\textbf{z}$ and learn $\\theta$, given the observed dataset $\\mathcal{D}$. The algorithm must work in the worst case scenario where the posterior distribution $p_\\theta(\\textbf{z}|\\textbf{x})$ is intractable, the integral of the marginal likelihood $p_\\theta(\\textbf{x})$ is intractable, and where $\\mathcal{D}$ is too large that sampling based solutions are not feasible.\nAuthors introduce the recognition model $q_\\phi(\\textbf{z}|\\textbf{x})$ to approximate the true intractable posterior $p_\\theta(\\textbf{z}|\\textbf{x})$. This can be seen as a probabilistic encoder that maps the data $\\textbf{x}$ to a distribution over the latent space $\\textbf{z}$. Similarly, the generative model $p_\\theta(\\textbf{x}|\\textbf{z})$ is a probabilistic decoder that maps the latent variable $\\textbf{z}$ to a distribution over the data space $\\textbf{x}$.\nThe Variational Lower Bound Since $\\mathcal{D}$ is i.i.d., the marginal log-likelihood of the data can be written as: $$ \\log p_\\theta(\\textbf{x}^{(1)}, \\dots, \\textbf{x}^{(N)}) = \\sum_{i=1}^N \\log p_\\theta(\\textbf{x}^{(i)}) $$ which given the recognition model can be rewritten using: $$ \\log p_\\theta(\\textbf{x}^{(i)}) = D_{KL}(q_\\phi(\\textbf{z}|\\textbf{x}^{(i)}) || p_\\theta(\\textbf{z}|\\textbf{x}^{(i)})) + \\mathcal{L}(\\theta, \\phi; \\textbf{x}^{(i)}) $$ where $D_{KL}$ is the Kullback-Leibler divergence between the recognition model and the true posterior, and $\\mathcal{L}$ is the variational lower bound, defined as: \\begin{align} \\log p_\\theta(\\textbf{x}^{(i)}) \\geq \\mathcal{L}(\\theta, \\phi; \\textbf{x}^{(i)}) =\u0026 \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})}[\\log p_\\theta(\\textbf{x}^{(i)},\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})] \\\\ =\u0026 \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})}[\\log p_\\theta(\\textbf{x}^{(i)}|\\textbf{z})] - D_{KL}(q_\\phi(\\textbf{z}|\\textbf{x}^{(i)}) || p_\\theta(\\textbf{z})) \\end{align} In eq. (2), the first term can be seen as the reconstruction error and the second term as the regularization term that ensures that the approximate posterior $q_\\phi(\\textbf{z}|\\textbf{x})$ is close to the prior $p_\\theta(\\textbf{z})$. We want to maximize the variational lower bound, w.r.t. the variational parameters $\\phi$ and the generative parameters $\\theta$. However, the gradient w.r.t. $\\phi$ is problematic.\nSGVB Estimator and AEVB Algorithm Kingma and Welling introduce SGVB as a practical estimator of the lower bound and its derivatives w.r.t. the parameters. They introduce the reparameterization trick, where the idea is to reparameterize the random variable $\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{x})$ as a deterministic differentiable transformation $g_\\phi(\\textbf{x}, \\epsilon)$ of a random variable $\\epsilon \\sim p(\\epsilon)$ that is independent of the parameters $\\phi$ and $\\theta$. This allows to form Monte Carlo estimates of the lower bound in eq. (1), which they call the SGVB estimator: \\begin{align} \\mathcal{L}(\\theta, \\phi; \\textbf{x}^{(i)}) =\u0026 \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})}[\\log p_\\theta(\\textbf{x}^{(i)},\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})] \\\\ =\u0026 \\mathbb{E}_{p(\\epsilon)}[\\log p_\\theta(\\textbf{x}^{(i)}, g_\\phi(\\textbf{x}^{(i)}, \\epsilon)) - \\log q_\\phi(g_\\phi(\\textbf{x}^{(i)}, \\epsilon)|\\textbf{x}^{(i)})] \\\\ \\simeq\u0026 \\frac{1}{L} \\sum_{l=1}^L \\log p_\\theta(\\textbf{x}^{(i)}, \\textbf{z}^{(i,l)}) - \\log q_\\phi(\\textbf{z}^{(i,l)}|\\textbf{x}^{(i)}) \\\\ =:\u0026 \\mathcal{L}^{\\text{SGVB}}(\\theta, \\phi; \\textbf{x}^{(i)}) \\end{align} where $\\textbf{z}^{(i,l)} = g_\\phi(\\textbf{x}^{(i)}, \\epsilon^{(l)})$ and $\\epsilon^{(l)} \\sim p(\\epsilon)$. In the case where the KL-divergence term in eq. (2) can be solved analytically, this yields to a second version of the SGVB estimatior, which is more stable: $$ \\mathcal{L}^{\\text{SGVB’}}(\\theta, \\phi; \\textbf{x}^{(i)}) = \\frac{1}{L} \\bigg(\\sum_{l=1}^L \\log p_\\theta(\\textbf{x}^{(i)}|\\textbf{z}^{(i,l)})\\bigg) - D_{KL}(q_\\phi(\\textbf{z}|\\textbf{x}^{(i)}) || p_\\theta(\\textbf{z})) $$ In practice, for a big dataset with a large number of samples $N$, we can use mini-batches of size $M$ to compute the SGVB estimator. This leads to the AEVB algorithm, which is a stochastic optimization algorithm that computes the gradients of the SGVB estimator for each mini-batch and updates the parameters $\\theta$ and $\\phi$ using a gradient-based optimization algorithm.\nVAE As an example of their proposed method they introduce the VAE. In this setting, the latent variables are sampled from $p_\\theta(\\textbf{z}) = \\mathcal{N}(\\textbf{z}; 0, I)$. The generative model $p_\\theta(\\textbf{x}|\\textbf{z})$ is either a Gaussian (real-valued data) or a Bernoulli (binary data) distribution, whose distribution parameters are computed from $\\textbf{z}$ using a neural network. The recognition model $q_\\phi(\\textbf{z}|\\textbf{x})$ is also a neural network that outputs the parameters of the Gaussian distribution of $\\textbf{z}$ given $\\textbf{x}$. The VAE is trained using the AEVB algorithm, where the gradients of the SGVB estimator are computed using backpropagation through the recognition model and the generative model. The neural networks they use are Multilayer Perceptrons (MLPs) with one hidden layer.\nPersonal Thoughts To my understanding, the main contribution of this paper is the introduction of the SGVB estimator, which is a practical way to compute the gradients of the variational lower bound. This allows to use gradient-based optimization algorithms to optimize the parameters of the recognition model and the generative model. This is a very important contribution to the field of variational inference, as it allows to use deep learning models to approximate the posterior distribution of complex probabilistic models.\nThe VAE is a very interesting model that can be used for unsupervised learning, semi-supervised learning, and generative modeling. It is a very flexible model that can be used for a wide range of applications, such as image generation, text generation, and speech generation. The VAE is a very active area of research, and many extensions and improvements have been proposed since the publication of this paper.\nDespite the complex theoretical background, the paper is very well written and easy to understand. The authors provide a lot of details and explanations that make it easy to follow the derivations and the algorithms. The paper is also very well organized, with a clear introduction, a detailed explanation of the method, and a thorough evaluation of the results.\nReferences [1] Kingma, D. P., \u0026 Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv:1312.6114.\n","wordCount":"1092","inLanguage":"en","datePublished":"2024-04-10T12:34:09+02:00","dateModified":"2024-04-10T12:34:09+02:00","author":{"@type":"Person","name":"Àlex Pujol Vidal"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/"},"publisher":{"@type":"Organization","name":"Àlex","logo":{"@type":"ImageObject","url":"https://alex-pv01.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://alex-pv01.github.io/ accesskey=h title="Àlex (Alt + H)"><img src=https://alex-pv01.github.io/apple-touch-icon.png alt aria-label=logo height=35>Àlex</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://alex-pv01.github.io/about/ title=about><span>about</span></a></li><li><a href=https://alex-pv01.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=https://alex-pv01.github.io/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://www.countop.com/ title=countop.com><span>countop.com</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Notes on Auto Encoding Variational Bayes</h1><div class=post-meta>&lt;span title='2024-04-10 12:34:09 +0200 CEST'>April 10, 101012&lt;/span>&amp;nbsp;·&amp;nbsp;6 min&amp;nbsp;·&amp;nbsp;Àlex Pujol Vidal&nbsp;|&nbsp;<a href=https://github.com/alex-pv01/alex-pv01.github.io/tree/main/content/posts/papers/auto-encoding-variational-bayes.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p><em><strong>Disclaimer:</strong></em> <em>This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.</em></p><p>The following post is a comment on the paper <a href=#1>Auto Encoding Variational Bayes</a> by <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kingma,+D+P">Diederik P. Kingma</a> and <a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Welling,+M">Max Welling</a>.</p><p>What introduces their contributions is the following question:</p><blockquote><p>How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?</p></blockquote><ol><li><p>They show how a <strong>reparameterization</strong> of the variational lower bound yields a simple <strong>differentiable</strong> unbiased estimator of the lower bound, which they call <strong>Stochastic Gradient Variational Bayes (SGVB)</strong> estimator. It can be used for efficient approximate posterior inference and learning in directed probabilistic models with continuous latent variables.</p></li><li><p>For the case of i.i.d. dataset and continuous latent variables, they propose the <strong>Auto-Encoding Variational Bayes (AEVB)</strong> algorithm. They use the SGVB estimator to optimize a recognition model that allows for efficient approximate posterior inference. This model derives to a <strong>Variational Autoencoder (VAE)</strong>, which is a neural network-based recognition model.</p></li></ol><h3 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h3><h4 id=problem-statement>Problem Statement<a hidden class=anchor aria-hidden=true href=#problem-statement>#</a></h4><p>Given a dataset $\mathcal{D} = \{\textbf{x}^{(i)}\}_{i=1}^N$ of $N$ i.i.d. samples of some continuous or discrete variable $\textbf{x}$, and assuming that the data is generated by a random process, involving the continuous random variable $\textbf{z}$, the generative process consists of two steps:</p><ol><li><p>A value $\textbf{z}^{(i)}$ is generated from a prior distribution $p_{\theta^*}(\textbf{z})$.</p></li><li><p>A value $\textbf{x}^{(i)}$ is generated from a conditional distribution $p_{\theta^*}(\textbf{x}|\textbf{z})$.</p></li></ol><p>Parameters $\theta^*$ and the latent variables $\textbf{z}$ are unknown. The goal is to define an algorithm to find an approximation posterior inference of $\textbf{z}$ and learn $\theta$, given the observed dataset $\mathcal{D}$. The algorithm must work in the worst case scenario where the posterior distribution $p_\theta(\textbf{z}|\textbf{x})$ is intractable, the integral of the marginal likelihood $p_\theta(\textbf{x})$ is intractable, and where $\mathcal{D}$ is too large that sampling based solutions are not feasible.</p><p>Authors introduce the <strong>recognition model</strong> $q_\phi(\textbf{z}|\textbf{x})$ to approximate the true intractable posterior $p_\theta(\textbf{z}|\textbf{x})$. This can be seen as a probabilistic <strong>encoder</strong> that maps the data $\textbf{x}$ to a distribution over the <strong>latent space</strong> $\textbf{z}$. Similarly, the <strong>generative model</strong> $p_\theta(\textbf{x}|\textbf{z})$ is a probabilistic <strong>decoder</strong> that maps the latent variable $\textbf{z}$ to a distribution over the data space $\textbf{x}$.</p><h4 id=the-variational-lower-bound>The Variational Lower Bound<a hidden class=anchor aria-hidden=true href=#the-variational-lower-bound>#</a></h4><p>Since $\mathcal{D}$ is i.i.d., the marginal log-likelihood of the data can be written as:
$$ \log p_\theta(\textbf{x}^{(1)}, \dots, \textbf{x}^{(N)}) = \sum_{i=1}^N \log p_\theta(\textbf{x}^{(i)}) $$
which given the recognition model can be rewritten using:
$$ \log p_\theta(\textbf{x}^{(i)}) = D_{KL}(q_\phi(\textbf{z}|\textbf{x}^{(i)}) || p_\theta(\textbf{z}|\textbf{x}^{(i)})) + \mathcal{L}(\theta, \phi; \textbf{x}^{(i)}) $$
where $D_{KL}$ is the Kullback-Leibler divergence between the recognition model and the true posterior, and $\mathcal{L}$ is the <strong>variational lower bound</strong>, defined as:
\begin{align}
\log p_\theta(\textbf{x}^{(i)}) \geq \mathcal{L}(\theta, \phi; \textbf{x}^{(i)}) =& \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x}^{(i)})}[\log p_\theta(\textbf{x}^{(i)},\textbf{z}) - \log q_\phi(\textbf{z}|\textbf{x}^{(i)})]
\\
=& \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x}^{(i)})}[\log p_\theta(\textbf{x}^{(i)}|\textbf{z})] - D_{KL}(q_\phi(\textbf{z}|\textbf{x}^{(i)}) || p_\theta(\textbf{z}))
\end{align}
In eq. (2), the first term can be seen as the <strong>reconstruction error</strong> and the second term as the <strong>regularization term</strong> that ensures that the approximate posterior $q_\phi(\textbf{z}|\textbf{x})$ is close to the prior $p_\theta(\textbf{z})$. We want to maximize the variational lower bound, w.r.t. the variational parameters $\phi$ and the generative parameters $\theta$. However, the gradient w.r.t. $\phi$ is problematic.</p><h4 id=sgvb-estimator-and-aevb-algorithm>SGVB Estimator and AEVB Algorithm<a hidden class=anchor aria-hidden=true href=#sgvb-estimator-and-aevb-algorithm>#</a></h4><p>Kingma and Welling introduce SGVB as a practical <strong>estimator of the lower bound</strong> and its derivatives w.r.t. the parameters. They introduce the <strong>reparameterization trick</strong>, where the idea is to reparameterize the random variable $\textbf{z}\sim q_\phi(\textbf{z}|\textbf{x})$ as a deterministic differentiable transformation $g_\phi(\textbf{x}, \epsilon)$ of a random variable $\epsilon \sim p(\epsilon)$ that is independent of the parameters $\phi$ and $\theta$. This allows to form Monte Carlo estimates of the lower bound in eq. (1), which they call the <strong>SGVB estimator</strong>:
\begin{align}
\mathcal{L}(\theta, \phi; \textbf{x}^{(i)}) =& \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x}^{(i)})}[\log p_\theta(\textbf{x}^{(i)},\textbf{z}) - \log q_\phi(\textbf{z}|\textbf{x}^{(i)})]
\\
=& \mathbb{E}_{p(\epsilon)}[\log p_\theta(\textbf{x}^{(i)}, g_\phi(\textbf{x}^{(i)}, \epsilon)) - \log q_\phi(g_\phi(\textbf{x}^{(i)}, \epsilon)|\textbf{x}^{(i)})]
\\
\simeq& \frac{1}{L} \sum_{l=1}^L \log p_\theta(\textbf{x}^{(i)}, \textbf{z}^{(i,l)}) - \log q_\phi(\textbf{z}^{(i,l)}|\textbf{x}^{(i)})
\\
=:& \mathcal{L}^{\text{SGVB}}(\theta, \phi; \textbf{x}^{(i)})
\end{align}
where $\textbf{z}^{(i,l)} = g_\phi(\textbf{x}^{(i)}, \epsilon^{(l)})$ and $\epsilon^{(l)} \sim p(\epsilon)$. In the case where the KL-divergence term in eq. (2) can be solved analytically, this yields to a second version of the SGVB estimatior, which is more stable:
$$ \mathcal{L}^{\text{SGVB&rsquo;}}(\theta, \phi; \textbf{x}^{(i)}) = \frac{1}{L} \bigg(\sum_{l=1}^L \log p_\theta(\textbf{x}^{(i)}|\textbf{z}^{(i,l)})\bigg) - D_{KL}(q_\phi(\textbf{z}|\textbf{x}^{(i)}) || p_\theta(\textbf{z})) $$
In practice, for a big dataset with a large number of samples $N$, we can use mini-batches of size $M$ to compute the SGVB estimator. This leads to the <strong>AEVB algorithm</strong>, which is a stochastic optimization algorithm that computes the gradients of the SGVB estimator for each mini-batch and updates the parameters $\theta$ and $\phi$ using a gradient-based optimization algorithm.</p><h4 id=vae>VAE<a hidden class=anchor aria-hidden=true href=#vae>#</a></h4><p>As an example of their proposed method they introduce the <strong>VAE</strong>. In this setting, the latent variables are sampled from $p_\theta(\textbf{z}) = \mathcal{N}(\textbf{z}; 0, I)$. The generative model $p_\theta(\textbf{x}|\textbf{z})$ is either a Gaussian (real-valued data) or a Bernoulli (binary data) distribution, whose distribution parameters are computed from $\textbf{z}$ using a neural network. The recognition model $q_\phi(\textbf{z}|\textbf{x})$ is also a neural network that outputs the parameters of the Gaussian distribution of $\textbf{z}$ given $\textbf{x}$. The VAE is trained using the AEVB algorithm, where the gradients of the SGVB estimator are computed using backpropagation through the recognition model and the generative model. The neural networks they use are Multilayer Perceptrons (MLPs) with one hidden layer.</p><h3 id=personal-thoughts>Personal Thoughts<a hidden class=anchor aria-hidden=true href=#personal-thoughts>#</a></h3><ul><li><p>To my understanding, the main contribution of this paper is the introduction of the SGVB estimator, which is a practical way to compute the gradients of the variational lower bound. This allows to use gradient-based optimization algorithms to optimize the parameters of the recognition model and the generative model. This is a very important contribution to the field of variational inference, as it allows to use deep learning models to approximate the posterior distribution of complex probabilistic models.</p></li><li><p>The VAE is a very interesting model that can be used for unsupervised learning, semi-supervised learning, and generative modeling. It is a very flexible model that can be used for a wide range of applications, such as image generation, text generation, and speech generation. The VAE is a very active area of research, and many extensions and improvements have been proposed since the publication of this paper.</p></li><li><p>Despite the complex theoretical background, the paper is very well written and easy to understand. The authors provide a lot of details and explanations that make it easy to follow the derivations and the algorithms. The paper is also very well organized, with a clear introduction, a detailed explanation of the method, and a thorough evaluation of the results.</p></li></ul><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>[1] Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. <a href=https://arxiv.org/abs/1312.6114>arXiv:1312.6114</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://alex-pv01.github.io/tags/variational-inference/>Variational Inference</a></li><li><a href=https://alex-pv01.github.io/tags/variational-autoencoder/>Variational Autoencoder</a></li><li><a href=https://alex-pv01.github.io/tags/autoencoder/>Autoencoder</a></li><li><a href=https://alex-pv01.github.io/tags/latent-space/>Latent Space</a></li><li><a href=https://alex-pv01.github.io/tags/sgvb/>SGVB</a></li><li><a href=https://alex-pv01.github.io/tags/aevb/>AEVB</a></li><li><a href=https://alex-pv01.github.io/tags/vae/>VAE</a></li><li><a href=https://alex-pv01.github.io/tags/ai/>AI</a></li><li><a href=https://alex-pv01.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://alex-pv01.github.io/tags/deep-learning/>Deep Learning</a></li><li><a href=https://alex-pv01.github.io/tags/probabilistic-models/>Probabilistic Models</a></li><li><a href=https://alex-pv01.github.io/tags/probabilistic-graphical-models/>Probabilistic Graphical Models</a></li><li><a href=https://alex-pv01.github.io/tags/pgm/>PGM</a></li><li><a href=https://alex-pv01.github.io/tags/directed-models/>Directed Models</a></li><li><a href=https://alex-pv01.github.io/tags/paper/>Paper</a></li><li><a href=https://alex-pv01.github.io/tags/research/>Research</a></li></ul><nav class=paginav><a class=prev href=https://alex-pv01.github.io/posts/papers/llava-gemma-accelerating-multimodal-foundation-models-with-a-compact-language-model/><span class=title>« Prev</span><br><span>Notes on LLaVA-Gemma: Accelerating Multimodal Foundation Models With a Compact Language Model</span>
</a><a class=next href=https://alex-pv01.github.io/posts/papers/ssse-efficiently-erasing-samples-from-trained-machine-learning-models/><span class=title>Next »</span><br><span>Notes on SSSE: Efficiently Erasing Samples From Trained Machine Learning Models</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on twitter" href="https://twitter.com/intent/tweet/?text=Notes%20on%20Auto%20Encoding%20Variational%20Bayes&amp;url=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f&amp;hashtags=VariationalInference%2cVariationalAutoencoder%2cAutoencoder%2cLatentSpace%2cSGVB%2cAEVB%2cVAE%2cAI%2cMachineLearning%2cDeepLearning%2cProbabilisticModels%2cProbabilisticGraphicalModels%2cPGM%2cDirectedModels%2cPaper%2cResearch"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f&amp;title=Notes%20on%20Auto%20Encoding%20Variational%20Bayes&amp;summary=Notes%20on%20Auto%20Encoding%20Variational%20Bayes&amp;source=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on reddit" href="https://reddit.com/submit?url=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f&title=Notes%20on%20Auto%20Encoding%20Variational%20Bayes"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on whatsapp" href="https://api.whatsapp.com/send?text=Notes%20on%20Auto%20Encoding%20Variational%20Bayes%20-%20https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on telegram" href="https://telegram.me/share/url?text=Notes%20on%20Auto%20Encoding%20Variational%20Bayes&amp;url=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Notes on Auto Encoding Variational Bayes on ycombinator" href="https://news.ycombinator.com/submitlink?t=Notes%20on%20Auto%20Encoding%20Variational%20Bayes&u=https%3a%2f%2falex-pv01.github.io%2fposts%2fpapers%2fauto-encoding-variational-bayes%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://alex-pv01.github.io/>Àlex</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>