[{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits by Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei.\nMa et. al. from Microsoft Research Asia and University of Chinese Academy of Sciences, are developing a new research line on 1-bit Large Language Models (LLMs) under the frame of General AI, whose mission is to advance AI for humanity. This is the second paper they have published in this regard.\nThis work is a follow up on ther previous publication where they introduced BitNet. The project is motivated by the increasing size of LLMs that pose challenges for deployment and raise concerns about the environmental impact due to the high energy coonsumption.\nAuthors claim that the new version introduced in this paper, BitNet b1.58 provide a Pareto solution to reduce inference cost of LLMs while maintaining the model performance. Thus, BitNet b1.58 enbales a new paradigm of LLM complexity and calls for actions to design new hardware optimized for 1-bit LLMs.\nBitNet b1.58 The main difference is in regard of the binarization step. There is no binarization anymore, now they adopt absmean quantization where the weights are set to be $+1, -1$ or $0$. This step is formalized as:\n\\begin{align} \\tilde{W} \u0026amp;= \\text{RoundClip}(\\frac{W}{\\gamma + \\epsilon}, -1, +1), \\\\ \\text{RoundClip}(x,a,b) \u0026amp;= \\max(a,\\min(b,\\text{round}(x))), \\\\ \\gamma \u0026amp;= \\frac{1}{nm}\\sum_{i,j}|W_{i,j}|. \\end{align}\nThe rest of the BitLinear layer remains the same as in the original BitNet.\nA good thing that they mention is that the method incorporate LLaMA-alike components so it is easier to implement them into the open-source frameworks.\nResults There is a dramatic improvement in terms of energy consumption and memory usage, while maintaining state-of-the-art performance. The following figures ilustrate this.\nDecoding latency (left) and memory consumption (right) of BitNet b1.58 varyiing the model size. Comparison of the throughput between BitNet b1.58 70B and LLaMA LLM 70B. Energy consumption of BitNet b1.58 compared to LLaMA LLM at 7nm process nodes. The components of arithmetic operations (left) and the end-to-end energy cost across different model sizes (right) Zero-shot accuracy comparison of BitNet b1.58 with StableLM-3B (SOTA open-source 3B model) with 2T tokens. BitNet achieves superior performance on all tasks, indication to have strong generalization capabilities. Personal thoughts I believe this paper is a game changer. They have unveiled a new paradigm, this is not only a new era of LLMs, but a new era of DNNs. I want to see 1-bit everything and everywhere! 1b-ResNet, 1b-YoLo, 1b-CNNs, etc.\nWhenever I see binary something I automatically think about sparsity. Is it possible to make the active weights sparse so that the model can benefit from sparsity to make it even more efficient?\nAnother thing that I already commented in the previous post about BitNet, is whether it would be possible to also binarize the data. Encoding the inputs into a binary space then performing binary operations with binary weights and finally debinarizing to get the sampling probabilities.\nReferences [1] Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., \u0026amp; Wei, F. (2024). The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits. arXiv:2402.17764.\n[2] Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., \u0026amp; Wei, F. (2023). Bitnet: Scaling 1 Bit Transformers for Large Language Models. arXiv:2310.11453.\n","permalink":"https://alex-pv01.github.io/posts/papers/the-era-of-1-bit-llms-all-large-language-models-are-in-1.58-bits/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits by Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei.","title":"Notes on The Era of 1-Bit LLMs: All Large Language Models Are in 1.58 Bits"},{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Bitnet: Scaling 1 Bit Transformers for Large Language Models by Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fang Yang, Ruiping Wang, Yi Wu, and Furu Wei.\nWang et. al. from Microsoft Research Asia, University of Chinese Academy of Sciences, and Tsinghua University, start what they call the era of 1-bit transformers by introducing BitNet. This project is framed under General AI, whose mission is to advance AI for humanity.\nTheir work is motivated by the increasing size of LLMs that pose challenges for deployment and raise concerns about the environmental impact due to the high energy coonsumption. A promising solution in these regards are model quantization techniques, where the weights of the models are quantized into set of lower precision. However, most existing quantization approaches for LLMs are post-training, which pose a significant loss in accuracy. The other approach is quantization-aware training, but in general the optimization step becomes more difficult to converge and is not clear whether they scale as good as transformers.\nBinarization is the extreme case of quantization. Authors investigate quantizatioin-aware training for 1-bit LLMs, introducing BitNet. In essence, the model employs low-precision binary weights and quantized activations, while maintaining high-precision for the optimizer states and gradients during training.\nBitNet Compared with vanilla Transformer, BitNet uses BitLinear, that uses 1-bit weights, instead of conventional matrix multiplication. The other components of the transformer are left to high-precision (8-bit in this case). Why?\nResidual connections and Layer Normalization costs are negligible The computation cost of QKV transformation is much smaller than the parametric projection as the model grows larger Input and output need high-precision to perform probability sampling Therefore, given a transformer we just need to change the nn.Linear layers by BitLinear.\nThe architecture of the BitNet, consisting of the stacks of attention layers and Feed-Forward Networks, where matrix multiplication is implemented as `BitLinear`. BitLinear First the weights are centralized to have zero-mean, then binarized to either $+1$ or $-1$ with the Sign function. A scaling $\\beta$ factor is used after binarization to reduce the $L_2$ error between the real-valued and the binarized weights.\nBesides the weights, the activation layer is also quantized to $b$-bit precision (8-bit in this case). They employ absmax quantization (which scales the activations into the range $[-Q_b, +Q_b]$) and layer normalization to do so.\nAll this in mind the BitLinear step is formulated as:\n\\begin{align} y \u0026amp;= \\tilde{W}\\tilde{x} = \\text{Sign}(W - \\alpha)\\text{Quant}(\\text{LN}(x)) \\times \\frac{\\beta \\gamma}{Q_b}, \\\\ \\alpha \u0026amp;= \\frac{1}{nm}\\sum_{i,j}W_{i,j}, \\\\ \\text{Quant}(x) \u0026amp;= \\text{Clip}(x \\times \\frac{Q_b}{\\gamma}, -Q_b+\\epsilon, Q_b - \\epsilon), \\\\ \\text{Clip}(x,a,b) \u0026amp;= \\max(a, \\min(b,x)), \\\\ \\gamma \u0026amp;= ||x||_\\infty, \\\\ \\text{LN}(x) \u0026amp;= \\frac{x-E(x)}{\\sqrt{\\text{Var}(x)+\\epsilon}}, \\\\ \\beta \u0026amp;= \\frac{1}{nm}||W||_1 \\end{align}\nThe computation flow of the `BitLinear` block. It is important to note that parameters $\\alpha, \\beta$ and $\\gamma$ need the entire tensor making parallelization impossible. To avoid this, Wang et. al. employ group quantization, dividing the weights and activations into groups and then independently estimate each group\u0026rsquo;s parameters.\nTraining Straight-through estimator: To pass the gradient between non differentiable steps (i.e., Sing and Clip) in the backward pass.\nMixed precision training: The gradients and optimizer are stored in high-precision to ensure stability and accuracy. During training there are latent weights in high-precision that accumulate the updates and then are binarized during the forward pass.\nLearning rate: Small updates on latent weights do not make any difference in 1-bit weights. They propose increasing the learning rate.\nComputational efficiency There is a huge saving in computational-energy consumption:\nEnergy consumption of BitNet against a vanilla Transformer varying different model sizes. 7nm and 45nm are two types of processes nodes. ADD stands for addition and MUL for multiplication of digits, in each of the WBits (weight bits) basis. Scaling curve against inference energy cost at 7nm process nodes vs scaling curve against model size. When considering the energy cost there is a dramatic improvement. Accuracy vs energy consumption of BitNet and FP16 Transformer. Zero-shot (left) and few-shot (right) performance of BitNet and FP16 Transformer against inference energy consumption. Comparison with Post-training Quantization The results demonstrate the effectiveness of BitNet in achieving competitive performance levels compared to the baseline approaches, particularly for lower bit levels.\nZero-shot results for BitNet and the other baselines. PTQ indicates Post-training quantization. WG (Winograd), WGe (Winogrande), HS (Hellaswag) and SC (Storycloze) are four different benchmarking datasets. PPL stands for Perplexity on the validation set. All models are of size 6.7B parameters. Personal thoughts Amazing work by GeneralAI team! A promising step to real democratization of LLMs and definitely a must research direction!\nNotice that not all the parameters of the model are binarized. This should be further explored and see whether it is feasible or not to binarize everything.\nI am wondering, could be possible to also biinarize the tokens? So instead of one-hot encodings they are binary vectors. This should be a richer \u0026ldquo;token\u0026rdquo; space that maybe suit well in a 1-bit LLM.\nReferences [1] Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., \u0026amp; Wei, F. (2023). Bitnet: Scaling 1 Bit Transformers for Large Language Models. arXiv:2310.11453.\n","permalink":"https://alex-pv01.github.io/posts/papers/bitnet-scaling-1-bit-transformers-for-large-language-models/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Bitnet: Scaling 1 Bit Transformers for Large Language Models by Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fang Yang, Ruiping Wang, Yi Wu, and Furu Wei.","title":"Notes on Bitnet: Scaling 1 Bit Transformers for Large Language Models"},{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper LlaVA-Gemma: Accelerating Multimodal Foundation Models With a Compact Language Model by Musashi Hinck, Matthew L. Olson, David Cobbley, Shao-Yen Tseng, and Vasudev Lal.\nHinck et. al. who are part of the Cognitive AI research area at Intel Labs, train a suite of Multimodal Foundation Models (MFM) using the LLaVA framework with the recently released Gemma Large Language Models (LLM). There is an increasing interest in small and capable Vision Language Models (VLM) and their proposed model, LLaVA-Gemma, is a step towards this direction. They provide an ablation study on three design features of the model:\nUtilizing a more powerful image backbone: The original LLaVA model uses CLIP as a pretrained vision encoder. They propose to use a larger and more powerful vision encoder, the DINOv2. Both models are compared in terms of GQA, MME, MM-Vet, POPE, VQAv2, and, MMVP, which are common benchmarking tools to other LLM works. The DINOv2 model shows better performance in general.\nIncreasing the size of the language backbone: In the case of the language encoder, LLaVA uses Llama-2, whereas Hinck et. al. use the recently released Gemma model. It comes in a variety of sizes, of which they use the smaller ones, Gemma-2B and Gemma-7B. The main difference between Llama and Gemma is that the latter uses a larger token set than any other LLM, that is 256k tokens (vs 50k tokens in Llama). However, their experiments show that the larger token set does not improve the performance of the model, in fact, it shows a decrease in performance (in terms of the above mentioned benchmarks). Since there is no a priori reason to expect this behaviour, they suggest that understanding the reasons behind this could be a fruitful area for future research.\nPretraining the connector: Other studies have shown that pretraining the connector, which is a MLP that maps the image features to the language features, can downstream performance. Contrary to this, their experiments show that pretraining the connector do improve the performance of the model.\nA deeper analysis between Gemma-2B and Gemma-7B shows that the larger model does\nOverall, the proposed LLaVA-Gemma falls short of performance compared to the SOTA models, but it is a step towards the democratization of VLMs. The good part is that authors have made the code and models available in the Hugging Face repository!\nReferences [1] Hinck, M., Olson, M. L., Cobbley, D., Tseng, S.-Y., \u0026amp; Lal, V. (2024). LlaVA-Gemma: Accelerating Multimodal Foundation Models With a Compact Language Model. arXiv:2404.01331.\n[2] Karamcheti, S., Nair, S., Balakrishna, A., Liang, P., Kollar, T., \u0026amp; Sadigh, D. (2024). Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models. arXiv:2402.07865.\n","permalink":"https://alex-pv01.github.io/posts/papers/llava-gemma-accelerating-multimodal-foundation-models-with-a-compact-language-model/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper LlaVA-Gemma: Accelerating Multimodal Foundation Models With a Compact Language Model by Musashi Hinck, Matthew L. Olson, David Cobbley, Shao-Yen Tseng, and Vasudev Lal.\nHinck et. al.","title":"Notes on LLaVA-Gemma: Accelerating Multimodal Foundation Models With a Compact Language Model"},{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.\nWhat introduces their contributions is the following question:\nHow can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?\nThey show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound, which they call Stochastic Gradient Variational Bayes (SGVB) estimator. It can be used for efficient approximate posterior inference and learning in directed probabilistic models with continuous latent variables.\nFor the case of i.i.d. dataset and continuous latent variables, they propose the Auto-Encoding Variational Bayes (AEVB) algorithm. They use the SGVB estimator to optimize a recognition model that allows for efficient approximate posterior inference. This model derives to a Variational Autoencoder (VAE), which is a neural network-based recognition model.\nMethod Problem Statement Given a dataset \\(\\mathcal{D} = \\{\\textbf{x}^{(i)}\\}_{i=1}^N\\) of $N$ i.i.d. samples of some continuous or discrete variable \\(\\textbf{x}\\), and assuming that the data is generated by a random process, involving the continuous random variable \\(\\textbf{z}\\), the generative process consists of two steps:\nA value \\(\\textbf{z}^{(i)}\\) is generated from a prior distribution \\(p_{\\theta^*}(\\textbf{z})\\).\nA value $\\textbf{x}^{(i)}$ is generated from a conditional distribution $p_{\\theta^*}(\\textbf{x}|\\textbf{z})$.\nParameters $\\theta^*$ and the latent variables $\\textbf{z}$ are unknown. The goal is to define an algorithm to find an approximation posterior inference of $\\textbf{z}$ and learn $\\theta$, given the observed dataset $\\mathcal{D}$. The algorithm must work in the worst case scenario where the posterior distribution $p_\\theta(\\textbf{z}|\\textbf{x})$ is intractable, the integral of the marginal likelihood $p_\\theta(\\textbf{x})$ is intractable, and where $\\mathcal{D}$ is too large that sampling based solutions are not feasible.\nAuthors introduce the recognition model $q_\\phi(\\textbf{z}|\\textbf{x})$ to approximate the true intractable posterior $p_\\theta(\\textbf{z}|\\textbf{x})$. This can be seen as a probabilistic encoder that maps the data $\\textbf{x}$ to a distribution over the latent space $\\textbf{z}$. Similarly, the generative model $p_\\theta(\\textbf{x}|\\textbf{z})$ is a probabilistic decoder that maps the latent variable $\\textbf{z}$ to a distribution over the data space $\\textbf{x}$.\nThe Variational Lower Bound Since $\\mathcal{D}$ is i.i.d., the marginal log-likelihood of the data can be written as: $$ \\log p_\\theta(\\textbf{x}^{(1)}, \\dots, \\textbf{x}^{(N)}) = \\sum_{i=1}^N \\log p_\\theta(\\textbf{x}^{(i)}) $$ which given the recognition model can be rewritten using: $$ \\log p_\\theta(\\textbf{x}^{(i)}) = D_{KL}(q_\\phi(\\textbf{z}|\\textbf{x}^{(i)}) || p_\\theta(\\textbf{z}|\\textbf{x}^{(i)})) + \\mathcal{L}(\\theta, \\phi; \\textbf{x}^{(i)}) $$ where $D_{KL}$ is the Kullback-Leibler divergence between the recognition model and the true posterior, and $\\mathcal{L}$ is the variational lower bound, defined as:\n\\begin{align} \\log p_\\theta(\\textbf{x}^{(i)}) \\geq \\mathcal{L}(\\theta, \\phi; \\textbf{x}^{(i)}) =\u0026amp; \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})}[\\log p_\\theta(\\textbf{x}^{(i)},\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})] \\\\ =\u0026amp; \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})}[\\log p_\\theta(\\textbf{x}^{(i)}|\\textbf{z})] - D_{KL}(q_\\phi(\\textbf{z}|\\textbf{x}^{(i)}) || p_\\theta(\\textbf{z})) \\end{align}\nIn eq. (2), the first term can be seen as the reconstruction error and the second term as the regularization term that ensures that the approximate posterior \\(q_\\phi(\\textbf{z}|\\textbf{x})\\) is close to the prior $p_\\theta(\\textbf{z})$. We want to maximize the variational lower bound, w.r.t. the variational parameters $\\phi$ and the generative parameters $\\theta$. However, the gradient w.r.t. $\\phi$ is problematic.\nSGVB Estimator and AEVB Algorithm Kingma and Welling introduce SGVB as a practical estimator of the lower bound and its derivatives w.r.t. the parameters. They introduce the reparameterization trick, where the idea is to reparameterize the random variable $\\textbf{z}\\sim q_\\phi(\\textbf{z}|\\textbf{x})$ as a deterministic differentiable transformation $g_\\phi(\\textbf{x}, \\epsilon)$ of a random variable $\\epsilon \\sim p(\\epsilon)$ that is independent of the parameters $\\phi$ and $\\theta$. This allows to form Monte Carlo estimates of the lower bound in eq. (1), which they call the SGVB estimator:\n$$\\begin{align} \\mathcal{L}(\\theta, \\phi; \\textbf{x}^{(i)}) =\u0026amp; \\mathbb{E}_{q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})}[\\log p_\\theta(\\textbf{x}^{(i)},\\textbf{z}) - \\log q_\\phi(\\textbf{z}|\\textbf{x}^{(i)})] \\\\ =\u0026amp; \\mathbb{E}_{p(\\epsilon)}[\\log p_\\theta(\\textbf{x}^{(i)}, g_\\phi(\\textbf{x}^{(i)}, \\epsilon)) - \\log q_\\phi(g_\\phi(\\textbf{x}^{(i)}, \\epsilon)|\\textbf{x}^{(i)})] \\\\ \\simeq\u0026amp; \\frac{1}{L} \\sum_{l=1}^L \\log p_\\theta(\\textbf{x}^{(i)}, \\textbf{z}^{(i,l)}) - \\log q_\\phi(\\textbf{z}^{(i,l)}|\\textbf{x}^{(i)}) \\\\ =:\u0026amp; \\mathcal{L}^{\\text{SGVB}}(\\theta, \\phi; \\textbf{x}^{(i)}) \\end{align}$$\nwhere $\\textbf{z}^{(i,l)} = g_\\phi(\\textbf{x}^{(i)}, \\epsilon^{(l)})$ and $\\epsilon^{(l)} \\sim p(\\epsilon)$. In the case where the KL-divergence term in eq. (2) can be solved analytically, this yields to a second version of the SGVB estimatior, which is more stable: $$ \\mathcal{L}^{\\text{SGVB\u0026rsquo;}}(\\theta, \\phi; \\textbf{x}^{(i)}) = \\frac{1}{L} \\bigg(\\sum_{l=1}^L \\log p_\\theta(\\textbf{x}^{(i)}|\\textbf{z}^{(i,l)})\\bigg) - D_{KL}(q_\\phi(\\textbf{z}|\\textbf{x}^{(i)}) || p_\\theta(\\textbf{z})) $$ In practice, for a big dataset with a large number of samples $N$, we can use mini-batches of size $M$ to compute the SGVB estimator. This leads to the AEVB algorithm, which is a stochastic optimization algorithm that computes the gradients of the SGVB estimator for each mini-batch and updates the parameters $\\theta$ and $\\phi$ using a gradient-based optimization algorithm.\nVAE As an example of their proposed method they introduce the VAE. In this setting, the latent variables are sampled from $p_\\theta(\\textbf{z}) = \\mathcal{N}(\\textbf{z}; 0, I)$. The generative model $p_\\theta(\\textbf{x}|\\textbf{z})$ is either a Gaussian (real-valued data) or a Bernoulli (binary data) distribution, whose distribution parameters are computed from $\\textbf{z}$ using a neural network. The recognition model $q_\\phi(\\textbf{z}|\\textbf{x})$ is also a neural network that outputs the parameters of the Gaussian distribution of $\\textbf{z}$ given $\\textbf{x}$. The VAE is trained using the AEVB algorithm, where the gradients of the SGVB estimator are computed using backpropagation through the recognition model and the generative model. The neural networks they use are Multilayer Perceptrons (MLPs) with one hidden layer.\nPersonal Thoughts To my understanding, the main contribution of this paper is the introduction of the SGVB estimator, which is a practical way to compute the gradients of the variational lower bound. This allows to use gradient-based optimization algorithms to optimize the parameters of the recognition model and the generative model. This is a very important contribution to the field of variational inference, as it allows to use deep learning models to approximate the posterior distribution of complex probabilistic models.\nThe VAE is a very interesting model that can be used for unsupervised learning, semi-supervised learning, and generative modeling. It is a very flexible model that can be used for a wide range of applications, such as image generation, text generation, and speech generation. The VAE is a very active area of research, and many extensions and improvements have been proposed since the publication of this paper.\nDespite the complex theoretical background, the paper is very well written and easy to understand. The authors provide a lot of details and explanations that make it easy to follow the derivations and the algorithms. The paper is also very well organized, with a clear introduction, a detailed explanation of the method, and a thorough evaluation of the results.\nReferences [1] Kingma, D. P., \u0026amp; Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv:1312.6114.\n","permalink":"https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.\nWhat introduces their contributions is the following question:\nHow can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?","title":"Notes on Auto Encoding Variational Bayes"},{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper SSSE: Efficiently Erasing Samples From Trained Machine Learning Models by Alexandra Peste, Dan Alistarh, and Christoph H. Lampert.\nPeste et. al. propose Single-Step Sample Erasure (SSSE), a method to efficiently and effectively erase samples from trained machine learning models. The removal step only requires access to the data to be erased, not the entire original training set.\nAlthough SSSE can be used for both convex and non-convex models, the main idea of the method comes from the optimal forgetting step in convex models. Let $f_{\\textbf{w}^}$ is the pre-trained model to be erased, and given a dataset $\\mathcal{D}$ together with $\\matchal{D}_f \\subset \\mathcal{D}$ and $\\mathcal{D}_r = \\mathcal{D} \\setminus \\mathcal{D}_f$, the data to forget and retain respectively. If we assume that the loss function $L$ is strictly convex, the optimal forgetting step is to find the optimal parameters $\\textbf{w}^_r$ that maximize the loss is given by: $$ \\textbf{w}^_r \\approx \\textbf{w}^ + \\frac{1}{n-k}H^{-1}_{\\mathcal{D}_r}(\\textbf{w}^) \\nabla_{\\textbf{w}} L_{\\mathcal{D}_f}(f_{\\textbf{w}}) $$ where $H_{\\mathcal{D}_r}(\\textbf{w}^)$ is the Hessian of the loss function $L$ at $\\textbf{w}^$ with respect to the data in $\\mathcal{D}_r$, and $n = |\\mathcal{D}|$ and $k = |\\mathcal{D}_f|$ are the total number of samples and the number of samples to be erased respectively, and $L_{\\mathcal{D}_f}(f_{\\textbf{w*}})$ is the loss of the model $f_{\\textbf{w*}}$ on the data $\\mathcal{D}_f$.\nSince the data to forget is much smaller than the entire dataset, the authors assume that the Hessian $H_{\\mathcal{D}r}(\\textbf{w}^)$ is approximately the same as $H_{\\mathcal{D}}(\\textbf{w}^)$. Computing the inverse of the Hessian is computationally expensive. Assuming that $L_{\\mathcal{D}} = \\sum{i=1}^n \\ell(f_{\\textbf{w}^}(\\textbf{x}^{(i)}), \\textbf{y}^{(i)}) = \\sum_{i=1}^n -\\log p(y_i|x_i; \\textbf{w}^)$, Peste et. al. propose to approximate it by the empirical Fisher Information Matrix (FIM), which is defined by: $$ F_\\mathcal{D}(\\textbf{w}^) = \\frac{1}{n} \\sum_{i=1}^n \\nabla_{\\textbf{w}} \\log p(\\textbf{y}^{(i)}|\\textbf{x}^{(i)};\\textbf{w}^) \\cdot \\nabla_{\\textbf{w}}^T \\log p(\\textbf{y}^{(i)}|\\textbf{x}^{(i)};\\textbf{w}^) $$ where $\\textbf{x}^{(i)}$ and $\\textbf{y}^{(i)}$ are the input and output of the model $f_{\\textbf{w}^}$ respectively. In practice this is computed efficiently with rank-1 updates by means of Sherman-Morrison lemma (#2). These defines the SSSE which is an approximation to the optimal forgetting step: $$ \\textbf{w}^_r \\approx \\textbf{w}^ + \\frac{\\epsilon}{n-k}F^{-1}_{\\mathcal{D}}(\\textbf{w}^) \\nabla_{\\textbf{w}} L_{\\mathcal{D}_f}(f_{\\textbf{w}}) $$ where $\\epsilon$ is a hyperparameter that controls the step size.\nTo properly choose $\\epsilon$, they define a similarity ratio to measure if the updated model is closer to the original model or to the retrained-from-scratch model. It is defined comparing the Area-Under-the-Curve (AUC) score corresponding to the Receiver Operating Characteristic (ROC) curve. In this setting they cannot conclude that SSSE converges precisely to the optimal forgetting step, but they show that it is effective in practice, not only for convex models but also for non-convex models.\nPersonal Thoughts As far as I understand, they are using the entire dataset to compute the empirical FIM, hence contradicting their claim that SSSE only requires access to the data to be erased. This is a bit confusing.\nThe theoretical analysis is quite interesting, and definitely studying the optimal forgetting step in convex models is a good direction. However, they assume need to assume that the loss is not only strictly convex but also that it is the negative log-likelihood of the data. This is a strong assumption that justifies their reasoning but they translate it to a more general setting only providing empirical evidence.\nIs nice that they introduce the similarity ratio to measure the performance of the method. There is a need for a metric to evaluate the performance of unlearning methods, and this is a good start.\nReference [1] Peste, A., Alistarh, D., \u0026amp; Lampert, C. H. (2021). SSSE: Efficiently Erasing Samples From Trained Machine Learning Models. arXiv:2107.03860.\n[2] Sherman, J., \u0026amp; Morrison, W. J. (1950). Adjustment of an Inverse Matrix Corresponding to Changes in the Elements of a Given Column or a Given Row of the Original Matrix. The Annals of Mathematical Statistics, 21(1), 124-127.\n","permalink":"https://alex-pv01.github.io/posts/papers/ssse-efficiently-erasing-samples-from-trained-machine-learning-models/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper SSSE: Efficiently Erasing Samples From Trained Machine Learning Models by Alexandra Peste, Dan Alistarh, and Christoph H. Lampert.\nPeste et. al. propose Single-Step Sample Erasure (SSSE), a method to efficiently and effectively erase samples from trained machine learning models.","title":"Notes on SSSE: Efficiently Erasing Samples From Trained Machine Learning Models"},{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Mixed-Privacy Forgetting in Deep Networks by Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto.\nGolatkar et. al. introduce a novel method for forgetting in a mixed-privacy setting, where a core subset of the training samples will not be forgotten. Their method allow efficient removal of all non-core (a.k.a. user) data by simply setting to zero a subset of the weights of the model, with minimal loss in performance. To do so, they introduce Mixed-Linear Forgetting (ML-Forgetting), which they claim to be the first algorithm to achieve forgettiing for deep networks trained on large-scale computer vision problems without compromising the accuracy.\nContributions Introduce the problem of mixed-privacy forgetting in deep networks. Propose ML-Forgetting, that trains a set of non-linear core weights and a set of linear user weights. By-design, ML-Forgetting allows to forget user all data by setting the user weights to zero. First algorithm to achieve forgetting for deep networks trained on large-scale computer vision problems without compromising the accuracy. Can handle multiple sequential forgetting requests, as well as class removal. Mixed-Linear Forgetting The main idea behind the method lies in the concept of quadratic forgetting, which comes from forgetting from a linear regression model, that has a quadratic loss function. User data is learned using such loss function, taking advantage of its convexity. First, they introduce the Mixed-Linear model, and then discuss the forgetting mechanism.\nMixed-Linear Model Two separate minimization problems are solved, one for the core data ($\\mathcal{D}_c$) and one for the user data ($\\mathcal{D}_u$). If $f_{\\textbf{w}}$ is the model with parameters $\\textbf{w}$, we have:\n$$\\textbf{w}_c^* = \\arg\\min_{\\textbf{w}_c} L_{\\mathcal{D_c}}(f_{\\textbf{w}_c})$$ $$\\textbf{w}_u^* = \\arg\\min_{\\textbf{w}_u} L_{\\mathcal{D_u}}(f_{\\textbf{w}_c^*+\\textbf{w}_u})$$\nwhere $L_{\\mathcal{D}}$ is the loss function for the dataset $\\mathcal{D}$. Since the deep network $f_{\\textbf{w}}$ is non-linear, the loss function $L_{\\mathcal{D}_u}(f_{\\textbf{w}_c^*+\\textbf{w}_u})$ can be highly non-convex. In light of [2], if $\\textbf{w}_u$ is a small perturbation, we can hope for a linear approximation $f_{\\textbf{w}}$ around $f_{\\textbf{w}_c^*}$, to have a similar performance to fine-tuning the entire model. Thus, the Mixed-Linear model is defined as the first-order Taylor expansion:\n$$f^{\\text{ML}}_{\\textbf{w}_c^*+\\textbf{w}_u} (\\textbf{x}) = f_{\\textbf{w}_c^*}(\\textbf{x}) + \\nabla_w f_{\\textbf{w}_c^*}(\\textbf{x}) \\cdot \\textbf{w}_u$$\nFurthermore, they use Cross-Entropy loss and Mean Squared Error loss, leading to the following minimization problem:\n$$\\textbf{w}_c^* = \\arg\\min_{\\textbf{w}_c} L^{\\text{CE}}_{\\mathcal{D_c}}(f_{\\textbf{w}_c})$$ $$\\textbf{w}_u^* = \\arg\\min_{\\textbf{w}_u} L^{\\text{MSE}}_{\\mathcal{D_u}}(f^{\\text{ML}}_{\\textbf{w}_c^*+\\textbf{w}_u})$$\nThe MSE loss has the advantage that the weights $\\textbf{w}_u$ are the solution of a quadratic minimization problem, which can be solved in closed form.\nForgetting Mechanism As seen in [3] and [4], in the case of the quadratic training loss, the optimal forgetting step to delete $\\mathcal{D}_f \\subset \\mathcal{D}$ is given by: $$\\textbf{w}_u \\mapsto \\textbf{w}_u - \\Delta\\textbf{w}_u + \\sigma^2 \\epsilon$$ where $\\Delta\\textbf{w}_u = H^{-1}_{\\mathcal{D}_r}(\\textbf{w}_c)\\nabla_\\textbf{w}L_{\\mathcal{D}_r}(f_{\\textbf{w}_u})$ is the optimal forgetting step, $H_{\\mathcal{D}_r}(\\textbf{w}_c)$ is the Hessian of the loss function $L_{\\mathcal{D}_r}$, $\\mathcal{D}_r=\\mathcal{D}-\\mathcal{D}_f$ is the retained data, and $\\epsilon \\sim N(0,I)$ is a random noise vector. As $\\Delta\\textbf{w}_u$ is only an approximation of the optimal forgetting step, by adding noise, they can destroy the information that may leak. In practice is not feasible to compute the Hessian, so they use the Jacobian-Vector Product (JVP) instead (see [2]).\nPersonal Thoughts Although the method is interesting, I am not sure how practical it is. The theoretical framework heavily relies on the assumption that the perturbation $\\textbf{w}_u$ is small, which may not be the case in practice. I find useful the fact of using core data to train a \u0026ldquo;foundational\u0026rdquo; (or core) model and then fine-tune it with user data (actually, this is the trend in SOTA models e.g., for generative AI). However, if the user data is far from being \u0026ldquo;small enough\u0026rdquo; and because of the linear approximation, the method may not work as expected. References [1] Golatkar, A., Achille, A., Ravichandran, A., Polito, M., \u0026amp; Soatto, S. (2021). Mixed-Privacy Forgetting in Deep Networks arXiv:2012.13431.\n[2] Mu, F., Liang, Y., \u0026amp; Li, Y. (2020). Gradients as features for deep representation learning. arXiv:2004.05529\n[3] Guo, C., Goldstein, T., Hannun, A., \u0026amp; Van Der Maaten, L. (2020). Certified data removal from machine learning models arXiv:911.03030\n[4] Golatkar, A., Achille, A., \u0026amp; Soatto, S. (2020). Eternal sunshine of the spotless net: Selective forgetting in deep networks arXiv:1911.04933\n","permalink":"https://alex-pv01.github.io/posts/papers/mixed-privacy-forgetting-in-deep-networks/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Mixed-Privacy Forgetting in Deep Networks by Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto.\nGolatkar et. al. introduce a novel method for forgetting in a mixed-privacy setting, where a core subset of the training samples will not be forgotten.","title":"Notes on Mixed-Privacy Forgetting in Deep Networks"},{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Multi Class Explainable Unlearning for Image Classification via Weight Filtering by Samuele Poppi, Sara Sarto, Marcella Cornia, Lorenzo Baraldi and Rita Cucchiara.\nSamuele P., et. al. propose a novel approach to unlearn a multiple classes from a pre-trained image classification model in a single untraining round. The technique learns a \u0026ldquo;Weight Filtering Network\u0026rdquo; (WF-Net) that is able to modulate the inner components of the model to remove the class of interest. The method discovers the weights that are responsible for the classification of the target class and then filters them out. This approach implicitly discovers the underlying relationships between network inner components and output classes and therefore allows to obtain a representation that can be employed for explainability purposes.\nIn comparison with single-class unlearning, WF-Net avoids the need of storing multiple models and performing multiple untraining rounds. This allows for a significant reduction in computational costs and memory usage, both at untraining and testing stages and provides increased flexibility.\nThe key observation is that there is a mapping between the inner components of a network and the output classes, as stated in [2]. Once trained, WF-Net is able to turn on and off those inner components to accomplish the desired unlearning behaviour on a class of choice. In practice, each layer $l$ of the pre-trained model is point-wise multiplied by a Weight Filtering (WF) layer $\\alpha_l$, which allows to modulate the weights of the model. The WF-Net, which is the sequence of all WF layers $\\alpha:=\\{\\alpha_l\\}_l$, is trained to remove a number of classes $N_c$ from the model. Note that $\\alpha_l$ has shape $N_c \\times K$, where $K$ is the length of $w_l$. After a single untraining round we end up with a single checkpoint of the WF-Net that can be used to instruct the model to behave as if any of the $N_c$ classes were never learned. It is possible to forget all classes at same time by setting $N_c$ to the total number of classes in the model. There are three key aspects regarding the training of the WF-Net:\nLoss function: Is composed of two terms, an unlearning loss $L_f$ and a retaining loss $L_r$. Both are implemented as cross-entropy losses. The total loss should be minimized zeroing $L_r(\\cdot)$ while maximizing $L_f(\\cdot)$: $$ L = \\lambda_0 \\sum_{(x,y)\\in\\mathcal{D_r}} L_r(M(x),y) + \\lambda_1 \\sum_{(x,y)\\in\\mathcal{D_f}} \\frac{1}{L_f(M(x),y)}$$ where $\\mathcal{D_r}$ and $\\mathcal{D_f}$ are the datasets of the classes to be retained and forgotten, respectively, $\\lambda_0$ and $\\lambda_1$ are hyperparameters that control the importance of the two terms, and $M$ is the WF model, i.e. the pre-trained model with fixed weights together with the WF-Net.\nRegularization: Adding a regularizer $R(\\cdot)$ to ensure only few parameters of $\\alpha_l$ are dropped to zero. $R(\\hat\\alpha)$ is computed as the average of inverted alphas $\\hat\\alpha_l:=1-\\alpha_l$: $$ L = \\lambda_0 \\sum_{(x,y)\\in\\mathcal{D_r}} L_r(M(x),y) + \\lambda_1 \\sum_{(x,y)\\in\\mathcal{D_f}} \\frac{1}{L_f(M(x),y)} + \\lambda_2 R(\\hat\\alpha)$$\nLabel expansion: To realize untraining of all classes simultaneously, during the training process, each mini-batch of size $B$ is divided into two halves, obtaining $B/2$ samples from the classes to be unlearned and $B/2$ samples from the classes to be retained. Samples from the first half are labeled with the original labels, while samples from the second half are randomly labeled. The random strategy is used to randomly retain one of the rows of each $\\alpha_l$. This retain step is performed $T$ times, pairing each time the samples with a different random label, and expanding the size of the retaining loss to $(T, B/2)$. The last step is averaging both losses.\nPersonal Thoughts Note that this approach is not really about unlearning, but about modulating the weights of the network using an additional network that is able to filter out the weights that are responsible for the classification of one (or more) of the $N_c$ classes to be forgotten. Despite that, it is an interesting method that can be used to improve the explainability models while acting \u0026ldquo;as if\u0026rdquo; the model has been untrained from a certain class.\nI am sceptical about the computational efficiency of the model. As far as I understand, the WF-Net has as many parameters as the pre-trained model, and expands the size of the mini-batches by a factor of $T$. This could lead to a significant increase in the computational cost of the training process of the WF-Net.\nReferences [1] Poppi, S., Sarto, S., Cornia, M., Baraldi, L., \u0026amp; Cucchiara, R. (2023). Multi Class Explainable Unlearning for Image Classification via Weight Filtering. arXiv:2304.02049.\n[2] Wang, A., Lee, W., \u0026amp; Qi, X. (2022). HINT: Hierarchical Neuron Concept Explainer. arXiv:2203.14196.\n","permalink":"https://alex-pv01.github.io/posts/papers/multi-class-explainable-unlearning-for-image-classification-via-weight-filtering/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a comment on the paper Multi Class Explainable Unlearning for Image Classification via Weight Filtering by Samuele Poppi, Sara Sarto, Marcella Cornia, Lorenzo Baraldi and Rita Cucchiara.\nSamuele P., et. al.","title":"Notes on Multi Class Explainable Unlearning for Image Classification via Weight Filtering"},{"content":"Attention mechanisms are a type of techniques used in natural language processing (NLP) tasks that allow a model to focus on specific parts of the input when processing a sequence, rather than considering the entire sequence at once. These methods can improve the performance of the model by allowing it to efficiently process long sequences of text and make more accurate predictions.\nTo some extend, attention mechanisms are motivated by how human visual attention focuses on different regions of an image or how correlates words in a sentence. They were born to deal with long sequences in sequence-to-sequence tasks. That is, any problem that requieres a sequence as an input, and outputs another sequence. For example, in machine translation, attention mechanisms can allow the model to translate each word in the source language sentence one at a time, focusing on the most relevant words in the source sentence at each step. This can be especially important for tasks that involve long sentences, as it allows the model to better capture the meaning and context of the input.\nThey have become a key component of many other tasks, not only in machine translation but also in other problems such as summarization, and language modeling. They have been shown to improve the performance of various models and are an active area of research in the field.\nHow attention mechanisms work: Attention mechanisms are often implemented as part of a recurrent neural network (RNN) in a sequence-to-sequence model, is compossed by an encoder and a decoder:\nThe encoder is a RNN that processes the input sequence and compresses the information into a context vector. Such vector represents the whole input sequence and is expected to be a good summary of its meaning.\nThe decoder is another RNN that recieves the context vector and outputs a transformed vector that ideally solves the problem we are dealing with.\nBy construction of the RNNs the context vector has a fixed size an requires the implementation of an attentenion mechanism to deal with long sequences, since otherwise the model \u0026ldquo;forgets\u0026rdquo; information.\nIn a broad sense, the seq2seq model processes the input sequence one element at a time. At each step, the attention mechanism calculates the weights for each element in the input sequence based on their relevance to the current state of the model. The weights can be calculated using various similarity measures, such as the dot product between the current state of the model and each element in the input sequence.\nMore formally, consider we have an input sequence $\\textbf{x}$ of lenght $n$ and that we want to output a target sequence $\\textbf{y}$ of lenght $m$,\n\\begin{align*} \\textbf{x} = [x_1, \\dots, x_n] \\\\ \\textbf{y} = [y_1, \\dots, y_m]. \\end{align*}\nWe start by initializing the hidden state of encoder, which can be a random vector, $\\bar{\\textbf{h}}_0$. While the sequence is not finished, it takes as input, one element at a time from $\\textbf{x}$ and the previous hidden state. At each step it generates a new vector $\\bar{\\textbf{h}}_i$ called hidden state of the encoder at step $i$, for $i = 1, \\dots, n$. Notice that each $\\bar{\\textbf{h}}_i$ is presumably more associated with the element $x_i$. Once, all the hidden states are processed, they are sent to the decoder.\nThe decoder also has its hidden state initialized, $\\textbf{h}_0$. Then, it takes at each step $t$ a weighted combination of the encoder hidden states as a current context vector, the previous decoder\u0026rsquo;s hidden state and the previous element of the output sequence to predict the value $y_t$. That is, for $t=1,\\dots, m$, the decoder\u0026rsquo;s hidden state at $t+1$ is of the form $\\textbf{h}_{t+1} = f(\\textbf{h}_t, \\textbf{c}_{t+1}, y_t)$, where:\n\\begin{align*} \\textbf{c}_t \u0026amp; = \\sum_{i=1}^n \\alpha_{t,i} \\bar{\\textbf{h}}_i \\quad \\hfill \u0026amp;; \\text{is the context vector at step }t. \\\\ \\alpha_{t+1,i} \u0026amp; = \\frac{\\exp(\\text{score}(\\textbf{h}_t, \\bar{\\textbf{h}}_i))}{\\sum_{i\u0026rsquo;=1}^n\\exp(\\text{score}(\\textbf{h}_t, \\bar{\\textbf{h}}_{i\u0026rsquo;}))} \\quad \\hfill \u0026amp;; \\text{softmaxed similarity score.} \\end{align*}\nThe $\\text{score}$ function assigns a measure of similarity between hidden states. There are several ways to approach it, first introduced by Bahdanau, et al., 2014 and Luong, et al., 2015. For instance, one could use as a simple similarity function the dot product. The value $\\alpha_{t,i}$ aims to indicate the similarity between element $y_t$ and $x_i$.\nFinally, in order to predict the value $\\textbf{y}_t$, the model concatenates the hidden layer $\\textbf{h}_t$ and the context vector $\\textbf{c}_t$, and pass them through a feedforward neural network that is trained simultaneously. This process is then repited until the completion of the output sequence.\nBahdanau attention: Bahdanau attention, also known as additive attention, is a type of attention mechanism that was introduced in a 2014 paper by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. It is widely used and has been shown to improve the performance of several models regarding NLP tasks.\nIt is introduced in the previous model by defining the following $\\text{score}$ function:\n$$ \\text{score}(\\textbf{h}_t, \\bar{\\textbf{h}}_i) = \\textbf{v}_a^T \\cdot \\text{tanh}(\\textbf{W}_a \\cdot [ \\textbf{h}_t ; \\bar{\\textbf{h}}_i ]) $$\nwhere $\\textbf{v}_a$ and $\\textbf{W}_a$ are weighted matrices to be learned in the training process. They can be implemented as dense layers using keras. For a python implementation of the Bahnadau attention mechanism one can be refered to the following Notebook.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class BahdanauAttention(tf.keras.layers.Layer): def __init__(self, units): super(BahdanauAttention, self).__init__() self.W1 = tf.keras.layers.Dense(units) self.W2 = tf.keras.layers.Dense(units) self.V = tf.keras.layers.Dense(1) def call(self, query, values): query_with_time_axis = tf.expand_dims(query, 1) # BAHDANAU Additive score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values))) # attention_weights shape == (batch_size, max_length, 1) attention_weights = tf.nn.softmax(score, axis=1) # context_vector shape after sum == (batch_size, hidden_size) context_vector = attention_weights * values context_vector = tf.reduce_sum(context_vector, axis=1) return context_vector, attention_weights Luong attention: Luong attention, introduced in a 2015 paper by Minh-Thang Luong et al., is a variant of Bahdanau attention that uses different similarity measures to calculate the attention weights.\nOne common variant is dot-product attention, which calculates the attention weights as the dot product between the current state of the model and each element in the input sequence. That is,\n$$ \\text{score}(\\textbf{h}_t, \\bar{\\textbf{h}}_i) = \\textbf{h}_t^T \\cdot \\bar{\\textbf{h}}_i $$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class LuongDotAttention(tf.keras.layers.Layer): def __init__(self): super(LuongDotAttention, self).__init__() def call(self, query, values): query_with_time_axis = tf.expand_dims(query, 1) values_transposed = tf.transpose(values, perm=[0, 2, 1]) # LUONG Dot-product score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed), perm=[0, 2, 1]) # attention_weights shape == (batch_size, max_length, 1) attention_weights = tf.nn.softmax(score, axis=1) # context_vector shape after sum == (batch_size, hidden_size) context_vector = attention_weights * values context_vector = tf.reduce_sum(context_vector, axis=1) return context_vector, attention_weights Another variant is general attention, which implements the attention weights $\\textbf{W}_a$ using a general linear function.\n$$ \\text{score}(\\textbf{h}_t, \\bar{\\textbf{h}}_i) = \\textbf{h}_t^T \\cdot \\textbf{W}_a \\cdot \\bar{\\textbf{h}}_i $$\nSuch $\\textbf{W}_a$ matrix is also to be learned during the training process. For a python implementation of the Luong attention mechanisms one can be refered to the following Notebook.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class LuongGeneralAttention(tf.keras.layers.Layer): def __init__(self, units): super(LuongGeneralAttention, self).__init__() self.W = tf.keras.layers.Dense(units) def call(self, query, values): query_with_time_axis = tf.expand_dims(query, 1) values_transposed = tf.transpose(values, perm=[0, 2, 1]) # LUONG General score = tf.transpose(tf.matmul(self.W(query_with_time_axis), values_transposed), perm=[0, 2, 1]) # attention_weights shape == (batch_size, max_length, 1) attention_weights = tf.nn.softmax(score, axis=1) # context_vector shape after sum == (batch_size, hidden_size) context_vector = attention_weights * values context_vector = tf.reduce_sum(context_vector, axis=1) return context_vector, attention_weights Limitations of attention mechanisms: While attention mechanisms have been shown to be effective in many NLP tasks, they do have some limitations:\nComputational intensity: Attention mechanisms can be computationally intensive, especially for large input sequences. This can make them difficult to train and use in practice, for instance on large datasets.\nLimited ability to capture long-range dependencies: Attention mechanisms can struggle to accurately capture long-range dependencies in the input, as they only consider the current state of the model and the input elements when calculating the attention weights. This can lead to suboptimal performance on tasks that require the model to consider the relationship between distant elements in the input sequence.\nLimited interpretability: Attention mechanisms can be difficult to interpret, as it is often not clear how the attention weights are being calculated or how they are influencing the model\u0026rsquo;s predictions. This can make it difficult to understand the decision-making process of the model and debug any errors.\nLimited generalizability: Attention mechanisms may not generalize well to new data, as they are trained on specific datasets and may not be able to adapt to different input distributions.\nConclusion: Overall, attention mechanisms have many strengths and have been shown to be effective in many NLP tasks. However, it is important to be aware of their limitations and to carefully consider whether they are the best approach for a particular task. Having said so, attention mechanisms are an active area of research in NLP and there are many potential directions for future development. These developments could lead to more efficient, accurate, interpretable, and generalizable attention mechanisms, which could further improve the performance of NLP models and enable them to solve more complex tasks.\n","permalink":"https://alex-pv01.github.io/posts/2022-12-17-attention/","summary":"Attention mechanisms are a type of techniques used in natural language processing (NLP) tasks that allow a model to focus on specific parts of the input when processing a sequence, rather than considering the entire sequence at once. These methods can improve the performance of the model by allowing it to efficiently process long sequences of text and make more accurate predictions.\nTo some extend, attention mechanisms are motivated by how human visual attention focuses on different regions of an image or how correlates words in a sentence.","title":"Attention and Context based Embeddings"}]