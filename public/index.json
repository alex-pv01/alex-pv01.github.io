[{"content":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a summary of the paper Deep Unsupervised Learning using Nonequilibrium Thermodynamics by Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan and Surya Ganguli. The paper was published in 2015 and it is the first one to introduce the concept of Diffusion Probabilistic Models (DPMs).\nBy that time, probabilistic models suffered from a conflicting trade-off between tractability and flexibility.\nTractable models are those that can be analytically evaluated and easily fit to data. However, they are not flexible enough to capture complex distributions. Flexible models can fit the structure of any distribution but they are hard to train, evaluate and sample from. They proposed Diffusion Probabilistic Models as a new family of probabilistic models that tackle this dichotomy and claim that DPMs allow:\nExtreme flexibility in model structure. Exact sampling. Easy multiplication with other distributions. Cheap evaluation of the log likelihood and probability of individual states. Diffusion Probabilistic Models The method is based on the idea of using a Markov chain to gradualy transform one distribution into another. In particular, they build a Markov chain which converts a simple known distribution, for instance a Gaussian, into the target complex data distribution using a diffusion process. Thus, learning involves estimating small perturbations within this process, which are more tractable than explicitly modeling the target distribution with a single intricate function.\n\u0026mdash;\u0026ndash; add image here \u0026mdash;\u0026ndash; caption: animation of a diffusion process, from a complex distribution to a simple one.\nAlgorithm The algorithm is defined as a two-step process, the forward trajectory and the reverse trajectory.\nForward trajectory Let us assume that the distribution of our data is $q(\\textbf{x}^{(0)})$. During this first stage, the algorithm will gradually transform $q(\\textbf{x}^{(0)})$ into a simple distribution $\\pi(\\textbf{y})$ by repeatingly applying a Markov diffusion kernel\n","permalink":"https://alex-pv01.github.io/posts/2023-12-18-about-dpm/","summary":"Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.\nThe following post is a summary of the paper Deep Unsupervised Learning using Nonequilibrium Thermodynamics by Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan and Surya Ganguli. The paper was published in 2015 and it is the first one to introduce the concept of Diffusion Probabilistic Models (DPMs).","title":"About Deep Unsupervised Learning Using Nonequilibrium Thermodynamics"}]