<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PGM on Àlex</title>
    <link>https://alex-pv01.github.io/tags/pgm/</link>
    <description>Recent content in PGM on Àlex</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 10 Apr 2024 12:34:09 +0200</lastBuildDate>
    <atom:link href="https://alex-pv01.github.io/tags/pgm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on Auto Encoding Variational Bayes</title>
      <link>https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/</link>
      <pubDate>Wed, 10 Apr 2024 12:34:09 +0200</pubDate>
      <guid>https://alex-pv01.github.io/posts/papers/auto-encoding-variational-bayes/</guid>
      <description>Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.&#xA;The following post is a comment on the paper Auto Encoding Variational Bayes by Diederik P. Kingma and Max Welling.&#xA;What introduces their contributions is the following question:&#xA;How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions?</description>
    </item>
    <item>
      <title>Notes on Denoising Diffusion Probabilistic Models</title>
      <link>https://alex-pv01.github.io/posts/papers/denoising-diffusion-probabilistic-models/</link>
      <pubDate>Tue, 19 Dec 2023 17:51:09 +0100</pubDate>
      <guid>https://alex-pv01.github.io/posts/papers/denoising-diffusion-probabilistic-models/</guid>
      <description>Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.&#xA;The following post is a summary of the paper Denoising Diffusion Probabilistic Models by Jonathan Ho, Ajay Jain and Pieter Abbeel, from University of California, Berkeley. The paper was published in 2020 and is a follow up on Diffusion Probabilistic Models (DPM).</description>
    </item>
    <item>
      <title>Notes on Deep Unsupervised Learning Using Nonequilibrium Thermodynamics</title>
      <link>https://alex-pv01.github.io/posts/papers/deep-unsupervised-learning-using-nonequilibrium-thermodynamics/</link>
      <pubDate>Mon, 18 Dec 2023 18:06:41 +0100</pubDate>
      <guid>https://alex-pv01.github.io/posts/papers/deep-unsupervised-learning-using-nonequilibrium-thermodynamics/</guid>
      <description>Disclaimer: This is part of my notes on AI research papers. I do this to learn and communicate what I understand. Feel free to comment if you have any suggestion, that would be very much appreciated.&#xA;The following post is a summary of the paper Deep Unsupervised Learning using Nonequilibrium Thermodynamics by Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan and Surya Ganguli, from Stanford University and University of California, Berkeley. The paper was published in 2015 and it is the first one to introduce the concept of Diffusion Probabilistic Models (DPMs) or, in short, Diffusion Models.</description>
    </item>
  </channel>
</rss>
